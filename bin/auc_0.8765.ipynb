{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"!curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n!python pytorch-xla-env-setup.py --apt-packages libomp5 libopenblas-dev","execution_count":1,"outputs":[{"output_type":"stream","text":"  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n100  3727  100  3727    0     0  88738      0 --:--:-- --:--:-- --:--:-- 88738\nUpdating TPU and VM. This may take around 2 minutes.\nUpdating TPU runtime to pytorch-dev20200325 ...\nFound existing installation: torch 1.4.0\nUninstalling torch-1.4.0:\n  Successfully uninstalled torch-1.4.0\nFound existing installation: torchvision 0.5.0\nUninstalling torchvision-0.5.0:\n  Successfully uninstalled torchvision-0.5.0\nCopying gs://tpu-pytorch/wheels/torch-nightly+20200325-cp36-cp36m-linux_x86_64.whl...\n- [1 files][ 83.4 MiB/ 83.4 MiB]                                                \nOperation completed over 1 objects/83.4 MiB.                                     \nDone updating TPU runtime: <Response [200]>\nCopying gs://tpu-pytorch/wheels/torch_xla-nightly+20200325-cp36-cp36m-linux_x86_64.whl...\n\\ [1 files][114.5 MiB/114.5 MiB]                                                \nOperation completed over 1 objects/114.5 MiB.                                    \nCopying gs://tpu-pytorch/wheels/torchvision-nightly+20200325-cp36-cp36m-linux_x86_64.whl...\n/ [1 files][  2.5 MiB/  2.5 MiB]                                                \nOperation completed over 1 objects/2.5 MiB.                                      \nProcessing ./torch-nightly+20200325-cp36-cp36m-linux_x86_64.whl\nRequirement already satisfied: future in /opt/conda/lib/python3.6/site-packages (from torch==nightly+20200325) (0.18.2)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.6/site-packages (from torch==nightly+20200325) (1.18.1)\n\u001b[31mERROR: fastai 1.0.60 requires torchvision, which is not installed.\u001b[0m\n\u001b[31mERROR: catalyst 20.2.4 requires torchvision>=0.2.1, which is not installed.\u001b[0m\n\u001b[31mERROR: allennlp 0.9.0 has requirement spacy<2.2,>=2.1.0, but you'll have spacy 2.2.3 which is incompatible.\u001b[0m\nInstalling collected packages: torch\nSuccessfully installed torch-1.5.0a0+d6149a7\nProcessing ./torch_xla-nightly+20200325-cp36-cp36m-linux_x86_64.whl\nInstalling collected packages: torch-xla\nSuccessfully installed torch-xla-1.6+e788e5b\nProcessing ./torchvision-nightly+20200325-cp36-cp36m-linux_x86_64.whl\nRequirement already satisfied: numpy in /opt/conda/lib/python3.6/site-packages (from torchvision==nightly+20200325) (1.18.1)\nRequirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from torchvision==nightly+20200325) (1.14.0)\nRequirement already satisfied: pillow>=4.1.1 in /opt/conda/lib/python3.6/site-packages (from torchvision==nightly+20200325) (5.4.1)\nRequirement already satisfied: torch in /opt/conda/lib/python3.6/site-packages (from torchvision==nightly+20200325) (1.5.0a0+d6149a7)\nRequirement already satisfied: future in /opt/conda/lib/python3.6/site-packages (from torch->torchvision==nightly+20200325) (0.18.2)\nInstalling collected packages: torchvision\nSuccessfully installed torchvision-0.6.0a0+3c254fb\nReading package lists... Done\nBuilding dependency tree       \nReading state information... Done\nThe following additional packages will be installed:\n  libopenblas-base\nThe following NEW packages will be installed:\n  libomp5 libopenblas-base libopenblas-dev\n0 upgraded, 3 newly installed, 0 to remove and 34 not upgraded.\nNeed to get 7831 kB of archives.\nAfter this operation, 92.2 MB of additional disk space will be used.\nGet:1 http://deb.debian.org/debian stretch/main amd64 libopenblas-base amd64 0.2.19-3 [3793 kB]\nGet:2 http://deb.debian.org/debian stretch/main amd64 libopenblas-dev amd64 0.2.19-3 [3809 kB]\nGet:3 http://deb.debian.org/debian stretch/main amd64 libomp5 amd64 3.9.1-1 [228 kB]\nFetched 7831 kB in 0s (58.4 MB/s) \ndebconf: delaying package configuration, since apt-utils is not installed\nSelecting previously unselected package libopenblas-base.\n(Reading database ... 74146 files and directories currently installed.)\nPreparing to unpack .../libopenblas-base_0.2.19-3_amd64.deb ...\nUnpacking libopenblas-base (0.2.19-3) ...\nSelecting previously unselected package libopenblas-dev.\nPreparing to unpack .../libopenblas-dev_0.2.19-3_amd64.deb ...\nUnpacking libopenblas-dev (0.2.19-3) ...\nSelecting previously unselected package libomp5:amd64.\nPreparing to unpack .../libomp5_3.9.1-1_amd64.deb ...\nUnpacking libomp5:amd64 (3.9.1-1) ...\nSetting up libomp5:amd64 (3.9.1-1) ...\nProcessing triggers for libc-bin (2.24-11+deb9u4) ...\nSetting up libopenblas-base (0.2.19-3) ...\nupdate-alternatives: using /usr/lib/openblas-base/libblas.so.3 to provide /usr/lib/libblas.so.3 (libblas.so.3) in auto mode\nupdate-alternatives: using /usr/lib/openblas-base/liblapack.so.3 to provide /usr/lib/liblapack.so.3 (liblapack.so.3) in auto mode\nSetting up libopenblas-dev (0.2.19-3) ...\nupdate-alternatives: using /usr/lib/openblas-base/libblas.so to provide /usr/lib/libblas.so (libblas.so) in auto mode\nupdate-alternatives: using /usr/lib/openblas-base/liblapack.so to provide /usr/lib/liblapack.so (liblapack.so) in auto mode\nProcessing triggers for libc-bin (2.24-11+deb9u4) ...\n","name":"stdout"}]},{"metadata":{"_uuid":"993abe0b-2561-4abc-a6a2-b83d8dd4c846","_cell_guid":"16a3df23-8416-46b5-8661-c52345005b6d","trusted":true},"cell_type":"code","source":"import os\nimport torch\nimport pandas as pd\nfrom scipy import stats\nimport numpy as np\n\nfrom tqdm import tqdm\nfrom collections import OrderedDict, namedtuple\nimport torch.nn as nn\nfrom torch.optim import lr_scheduler\nimport joblib\n\nimport logging\nimport transformers\nfrom transformers import AdamW, get_linear_schedule_with_warmup, get_constant_schedule\nimport sys\nfrom sklearn import metrics, model_selection\n\nimport warnings\nimport torch_xla\nimport torch_xla.debug.metrics as met\nimport torch_xla.distributed.data_parallel as dp\nimport torch_xla.distributed.parallel_loader as pl\nimport torch_xla.utils.utils as xu\nimport torch_xla.core.xla_model as xm\nimport torch_xla.distributed.xla_multiprocessing as xmp\nimport torch_xla.test.test_utils as test_utils\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")\n\nclass AverageMeter:\n    \"\"\"\n    Computes and stores the average and current value\n    \"\"\"\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n\n\nclass BERTBaseUncased(nn.Module):\n    def __init__(self, bert_path):\n        super(BERTBaseUncased, self).__init__()\n        self.bert_path = bert_path\n        self.bert = transformers.BertModel.from_pretrained(self.bert_path)\n        self.bert_drop = nn.Dropout(0.3)\n        self.out = nn.Linear(768 * 2, 1)\n\n    def forward(\n            self,\n            ids,\n            mask,\n            token_type_ids\n    ):\n        o1, o2 = self.bert(\n            ids,\n            attention_mask=mask,\n            token_type_ids=token_type_ids)\n        \n        apool = torch.mean(o1, 1)\n        mpool, _ = torch.max(o1, 1)\n        cat = torch.cat((apool, mpool), 1)\n\n        bo = self.bert_drop(cat)\n        p2 = self.out(bo)\n        return p2\n\n\nclass BERTDatasetTraining:\n    def __init__(self, comment_text, targets, tokenizer, max_length):\n        self.comment_text = comment_text\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n        self.targets = targets\n\n    def __len__(self):\n        return len(self.comment_text)\n\n    def __getitem__(self, item):\n        comment_text = str(self.comment_text[item])\n        comment_text = \" \".join(comment_text.split())\n\n        inputs = self.tokenizer.encode_plus(\n            comment_text,\n            None,\n            add_special_tokens=True,\n            max_length=self.max_length,\n        )\n        ids = inputs[\"input_ids\"]\n        token_type_ids = inputs[\"token_type_ids\"]\n        mask = inputs[\"attention_mask\"]\n        \n        padding_length = self.max_length - len(ids)\n        \n        ids = ids + ([0] * padding_length)\n        mask = mask + ([0] * padding_length)\n        token_type_ids = token_type_ids + ([0] * padding_length)\n        \n        return {\n            'ids': torch.tensor(ids, dtype=torch.long),\n            'mask': torch.tensor(mask, dtype=torch.long),\n            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n            'targets': torch.tensor(self.targets[item], dtype=torch.float)\n        }","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mx = BERTBaseUncased(bert_path=\"../input/bert-base-multilingual-uncased/\")\ndf_train1 = pd.read_csv(\"../input/jigsaw-multilingual-toxic-comment-classification/jigsaw-toxic-comment-train.csv\", usecols=[\"comment_text\", \"toxic\"]).fillna(\"none\")\ndf_train2 = pd.read_csv(\"../input/jigsaw-multilingual-toxic-comment-classification/jigsaw-unintended-bias-train.csv\", usecols=[\"comment_text\", \"toxic\"]).fillna(\"none\")\ndf_train_full = pd.concat([df_train1, df_train2], axis=0).reset_index(drop=True)\ndf_train = df_train_full.sample(frac=1, random_state=100).reset_index(drop=True).head(200_000)\n\ndf_valid = pd.read_csv('../input/jigsaw-multilingual-toxic-comment-classification/validation.csv', \n                       usecols=[\"comment_text\", \"toxic\"])\n\ndf_train = pd.concat([df_train, df_valid], axis=0).reset_index(drop=True)\ndf_train = df_train.sample(frac=1, random_state=100).reset_index(drop=True)","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def _run():\n    def loss_fn(outputs, targets):\n        return nn.BCEWithLogitsLoss()(outputs, targets.view(-1, 1))\n\n    def train_loop_fn(data_loader, model, optimizer, device, scheduler=None):\n        model.train()\n        for bi, d in enumerate(data_loader):\n            ids = d[\"ids\"]\n            mask = d[\"mask\"]\n            token_type_ids = d[\"token_type_ids\"]\n            targets = d[\"targets\"]\n\n            ids = ids.to(device, dtype=torch.long)\n            mask = mask.to(device, dtype=torch.long)\n            token_type_ids = token_type_ids.to(device, dtype=torch.long)\n            targets = targets.to(device, dtype=torch.float)\n\n            optimizer.zero_grad()\n            outputs = model(\n                ids=ids,\n                mask=mask,\n                token_type_ids=token_type_ids\n            )\n\n            loss = loss_fn(outputs, targets)\n            if bi % 10 == 0:\n                xm.master_print(f'bi={bi}, loss={loss}')\n\n            loss.backward()\n            xm.optimizer_step(optimizer)\n            if scheduler is not None:\n                scheduler.step()\n\n    def eval_loop_fn(data_loader, model, device):\n        model.eval()\n        fin_targets = []\n        fin_outputs = []\n        for bi, d in enumerate(data_loader):\n            ids = d[\"ids\"]\n            mask = d[\"mask\"]\n            token_type_ids = d[\"token_type_ids\"]\n            targets = d[\"targets\"]\n\n            ids = ids.to(device, dtype=torch.long)\n            mask = mask.to(device, dtype=torch.long)\n            token_type_ids = token_type_ids.to(device, dtype=torch.long)\n            targets = targets.to(device, dtype=torch.float)\n\n            outputs = model(\n                ids=ids,\n                mask=mask,\n                token_type_ids=token_type_ids\n            )\n\n            targets_np = targets.cpu().detach().numpy().tolist()\n            outputs_np = outputs.cpu().detach().numpy().tolist()\n            fin_targets.extend(targets_np)\n            fin_outputs.extend(outputs_np)    \n\n        return fin_outputs, fin_targets\n\n    \n    MAX_LEN = 160\n    TRAIN_BATCH_SIZE = 64\n    EPOCHS = 3\n\n    tokenizer = transformers.BertTokenizer.from_pretrained(\"../input/bert-base-multilingual-uncased/\", do_lower_case=True)\n\n    train_targets = df_train.toxic.values\n    valid_targets = df_valid.toxic.values\n\n    train_dataset = BERTDatasetTraining(\n        comment_text=df_train.comment_text.values,\n        targets=train_targets,\n        tokenizer=tokenizer,\n        max_length=MAX_LEN\n    )\n\n    train_sampler = torch.utils.data.distributed.DistributedSampler(\n          train_dataset,\n          num_replicas=xm.xrt_world_size(),\n          rank=xm.get_ordinal(),\n          shuffle=True)\n\n    train_data_loader = torch.utils.data.DataLoader(\n        train_dataset,\n        batch_size=TRAIN_BATCH_SIZE,\n        sampler=train_sampler,\n        drop_last=True,\n        num_workers=1\n    )\n\n    valid_dataset = BERTDatasetTraining(\n        comment_text=df_valid.comment_text.values,\n        targets=valid_targets,\n        tokenizer=tokenizer,\n        max_length=MAX_LEN\n    )\n\n    valid_sampler = torch.utils.data.distributed.DistributedSampler(\n          valid_dataset,\n          num_replicas=xm.xrt_world_size(),\n          rank=xm.get_ordinal(),\n          shuffle=False)\n\n    valid_data_loader = torch.utils.data.DataLoader(\n        valid_dataset,\n        batch_size=16,\n        sampler=valid_sampler,\n        drop_last=False,\n        num_workers=1\n    )\n\n    device = xm.xla_device()\n    model = mx.to(device)\n\n    param_optimizer = list(model.named_parameters())\n    no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n    optimizer_grouped_parameters = [\n        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.001},\n        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}]\n\n    lr = 0.4 * 1e-5 * xm.xrt_world_size()\n    num_train_steps = int(len(train_dataset) / TRAIN_BATCH_SIZE / xm.xrt_world_size() * EPOCHS)\n    xm.master_print(f'num_train_steps = {num_train_steps}, world_size={xm.xrt_world_size()}')\n\n    optimizer = AdamW(optimizer_grouped_parameters, lr=lr)\n    scheduler = get_linear_schedule_with_warmup(\n        optimizer,\n        num_warmup_steps=0,\n        num_training_steps=num_train_steps\n    )\n    \n    best_score = float('-inf')\n    for epoch in range(EPOCHS):\n        para_loader = pl.ParallelLoader(train_data_loader, [device])\n        train_loop_fn(para_loader.per_device_loader(device), model, optimizer, device, scheduler=scheduler)\n\n        para_loader = pl.ParallelLoader(valid_data_loader, [device])\n        o, t = eval_loop_fn(para_loader.per_device_loader(device), model, device)\n        xm.save(model.state_dict(), \"model.bin\")\n        auc_score = metrics.roc_auc_score(np.array(t) >= 0.5, o)\n        xm.master_print(f'AUC = {auc_score}')\n        if auc_score > best_score:\n            xm.save(model.state_dict(), \"best_model1.bin\")\n            best_score = auc_score\n            xm.master_print(\"New best score: %.5f\" % best_score)","execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Start training processes\ndef _mp_fn(rank, flags):\n    torch.set_default_tensor_type('torch.FloatTensor')\n    a = _run()\n\nFLAGS={}\nxmp.spawn(_mp_fn, args=(FLAGS,), nprocs=8, start_method='fork')","execution_count":6,"outputs":[{"output_type":"stream","text":"num_train_steps = 1218, world_size=8\nbi=0, loss=0.8665692806243896\nbi=10, loss=0.25665774941444397\nbi=20, loss=0.41240188479423523\nbi=30, loss=0.30111831426620483\nbi=40, loss=0.3163526952266693\nbi=50, loss=0.18885572254657745\nbi=60, loss=0.22321239113807678\nbi=70, loss=0.3323131203651428\nbi=80, loss=0.2211921364068985\nbi=90, loss=0.3333137035369873\nbi=100, loss=0.24058620631694794\nbi=110, loss=0.27529847621917725\nbi=120, loss=0.215770423412323\nbi=130, loss=0.2329595386981964\nbi=140, loss=0.29843899607658386\nbi=150, loss=0.277553528547287\nbi=160, loss=0.21837754547595978\nbi=170, loss=0.202921524643898\nbi=180, loss=0.31423473358154297\nbi=190, loss=0.19049808382987976\nbi=200, loss=0.20059137046337128\nbi=210, loss=0.30913880467414856\nbi=220, loss=0.3517529368400574\nbi=230, loss=0.2499007284641266\nbi=240, loss=0.2242073118686676\nbi=250, loss=0.2651684284210205\nbi=260, loss=0.17957015335559845\nbi=270, loss=0.18760722875595093\nbi=280, loss=0.2512918710708618\nbi=290, loss=0.20444199442863464\nbi=300, loss=0.21936628222465515\nbi=310, loss=0.19604675471782684\nbi=320, loss=0.24051624536514282\nbi=330, loss=0.24462465941905975\nbi=340, loss=0.1854843944311142\nbi=350, loss=0.18870052695274353\nbi=360, loss=0.19481392204761505\nbi=370, loss=0.19214123487472534\nbi=380, loss=0.21898339688777924\nbi=390, loss=0.25654539465904236\nbi=400, loss=0.320210725069046\nAUC = 0.9672978322949117\nNew best score: 0.96730\nbi=0, loss=0.21102890372276306\nbi=10, loss=0.1338980346918106\nbi=20, loss=0.2883206009864807\nbi=30, loss=0.2123800665140152\nbi=40, loss=0.2588868737220764\nbi=50, loss=0.1617766171693802\nbi=60, loss=0.21966952085494995\nbi=70, loss=0.2976396679878235\nbi=80, loss=0.1709391474723816\nbi=90, loss=0.24905692040920258\nbi=100, loss=0.2159900665283203\nbi=110, loss=0.2502352297306061\nbi=120, loss=0.21477743983268738\nbi=130, loss=0.2206893414258957\nbi=140, loss=0.2813393175601959\nbi=150, loss=0.23282256722450256\nbi=160, loss=0.21340788900852203\nbi=170, loss=0.19606633484363556\nbi=180, loss=0.267499715089798\nbi=190, loss=0.17717678844928741\nbi=200, loss=0.1988774836063385\nbi=210, loss=0.29293695092201233\nbi=220, loss=0.34613433480262756\nbi=230, loss=0.20253069698810577\nbi=240, loss=0.21014326810836792\nbi=250, loss=0.27392902970314026\nbi=260, loss=0.18126267194747925\nbi=270, loss=0.18782928586006165\nbi=280, loss=0.2470971941947937\nbi=290, loss=0.20571894943714142\nbi=300, loss=0.21950046718120575\nbi=310, loss=0.188479483127594\nbi=320, loss=0.23374512791633606\nbi=330, loss=0.22388644516468048\nbi=340, loss=0.19504253566265106\nbi=350, loss=0.18911297619342804\nbi=360, loss=0.18050940334796906\nbi=370, loss=0.1874525249004364\nbi=380, loss=0.18263716995716095\nbi=390, loss=0.2338417023420334\nbi=400, loss=0.31437066197395325\nAUC = 0.989705023364486\nNew best score: 0.98971\nbi=0, loss=0.19827266037464142\nbi=10, loss=0.12836496531963348\nbi=20, loss=0.2949991822242737\nbi=30, loss=0.19946163892745972\nbi=40, loss=0.22205235064029694\nbi=50, loss=0.15124215185642242\nbi=60, loss=0.21843509376049042\nbi=70, loss=0.27207228541374207\nbi=80, loss=0.15168370306491852\nbi=90, loss=0.2193831205368042\nbi=100, loss=0.1774447113275528\nbi=110, loss=0.24065810441970825\nbi=120, loss=0.21227310597896576\nbi=130, loss=0.22239950299263\nbi=140, loss=0.27431944012641907\nbi=150, loss=0.2181643843650818\nbi=160, loss=0.2098083794116974\nbi=170, loss=0.19911034405231476\nbi=180, loss=0.25467371940612793\nbi=190, loss=0.16994063556194305\nbi=200, loss=0.18785658478736877\nbi=210, loss=0.31491491198539734\nbi=220, loss=0.2982136607170105\nbi=230, loss=0.2020493596792221\nbi=240, loss=0.2086094170808792\nbi=250, loss=0.2765187919139862\nbi=260, loss=0.1612415313720703\nbi=270, loss=0.1604016125202179\nbi=280, loss=0.24037547409534454\nbi=290, loss=0.20831651985645294\nbi=300, loss=0.21015074849128723\nbi=310, loss=0.1854403167963028\nbi=320, loss=0.21657778322696686\nbi=330, loss=0.22329041361808777\nbi=340, loss=0.18388564884662628\nbi=350, loss=0.18833984434604645\nbi=360, loss=0.17242605984210968\nbi=370, loss=0.17978954315185547\nbi=380, loss=0.1689969152212143\nbi=390, loss=0.2311621606349945\nbi=400, loss=0.3056558072566986\nAUC = 0.9967143691588786\nNew best score: 0.99671\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}